import os
import requests
from bs4 import BeautifulSoup
import logging

# =======================
# üîß CONFIGURACI√ìN
# =======================
URL = os.getenv("URL")
KEYWORDS = [k.strip().lower() for k in os.getenv("KEYWORDS", "").split(",")]
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
CACHE_FILE = "sent_alerts.txt"

# =======================
# üß∞ LOGGING CONFIG
# =======================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# =======================
# üß† FUNCIONES AUXILIARES
# =======================
def load_cache():
    """Carga URLs ya enviadas desde el archivo de cach√©"""
    if not os.path.exists(CACHE_FILE):
        return set()
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())

def save_cache(sent_urls):
    """Guarda las URLs actualizadas en el archivo de cach√©"""
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        for url in sorted(sent_urls):
            f.write(url + "\n")

def send_telegram_message(text):
    """Env√≠a mensaje al canal de Telegram."""
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        logging.error("‚ùå TELEGRAM_BOT_TOKEN o TELEGRAM_CHAT_ID no configurados")
        return

    endpoint = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {"chat_id": TELEGRAM_CHAT_ID, "text": text, "parse_mode": "HTML"}
    response = requests.post(endpoint, data=payload)
    if response.status_code != 200:
        logging.error(f"‚ö†Ô∏è Error enviando mensaje: {response.status_code} - {response.text}")

def get_all_urls(site_url):
    """Obtiene todas las URLs de la p√°gina especificada."""
    try:
        response = requests.get(site_url, timeout=15)
        response.raise_for_status()
    except Exception as e:
        logging.error(f"‚ùå Error al obtener {site_url}: {e}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    urls = set()

    # Extrae todos los enlaces v√°lidos
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if href.startswith("http"):
            urls.add(href)
        elif href.startswith("/"):
            base = site_url.rstrip("/")
            urls.add(base + href)

    logging.info(f"üîó Se encontraron {len(urls)} URLs en {site_url}")
    return list(urls)

# =======================
# üöÄ FUNCI√ìN PRINCIPAL
# =======================
def main():
    logging.info("üöÄ Iniciando ejecuci√≥n del scraper...")
    if not URL or not KEYWORDS:
        logging.error("‚ùå Variables de entorno URL o KEYWORDS no configuradas.")
        return

    logging.info(f"üîç Analizando sitio: {URL}")
    logging.info(f"üß© Palabras clave: {KEYWORDS}")

    cache = load_cache()
    urls = get_all_urls(URL)
    new_alerts = []

    for link in urls:
        link_lower = link.lower()
        # Revisa TODAS las palabras clave
        if any(keyword in link_lower for keyword in KEYWORDS):
            if link not in cache:
                new_alerts.append(link)
                cache.add(link)

    if new_alerts:
        for url in new_alerts:
            message = f"üì∞ <b>Noticia detectada:</b>\n{url}"
            send_telegram_message(message)
            logging.info(f"üì¢ Enviada alerta: {url}")
        save_cache(cache)
        logging.info(f"‚úÖ {len(new_alerts)} nuevas alertas enviadas.")
    else:
        logging.info("üò¥ No se encontraron coincidencias nuevas.")

    logging.info("üèÅ Ejecuci√≥n completada.")

# =======================
# üèÉ‚Äç‚ôÇÔ∏è EJECUCI√ìN
# =======================
if __name__ == "__main__":
    main()
